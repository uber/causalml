{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "642f1ef4",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Multi-Armed Bandit (MAB) Demonstration\n",
    "\n",
    "This notebook demonstrates how to use the MAB (Multi-Armed Bandit) algorithms in `causalml` for:\n",
    "- Simulating bandit data\n",
    "- Running classical and contextual bandit algorithms\n",
    "- Evaluating and visualizing results\n",
    "\n",
    "We will walk through each step with explanations and code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1237d79c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import all the necessary libraries and define helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d7b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from causalml.dataset import make_mab_data\n",
    "from causalml.optimize.bandit import (\n",
    "    EpsilonGreedy, UCB, ThompsonSampling, BatchBandit,\n",
    "    LinUCB, CohortThompsonSampling, BatchLinUCB, BatchCohortThompsonSampling\n",
    ")\n",
    "from causalml.metrics import (\n",
    "    cumulative_reward, cumulative_regret,\n",
    "    plot_cumulative_reward, plot_cumulative_regret, plot_arm_selection_frequency\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2307f61b",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Helper Functions\n",
    "\n",
    "We define a helper function `run_online_bandit` that handles the online evaluation of different bandit algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f295a2e9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_online_bandit(algo, X, arms, rewards, context_cols=None, cohort_col=None):\n",
    "    \"\"\"\n",
    "    Run online evaluation of a bandit algorithm.\n",
    "    \n",
    "    Args:\n",
    "        algo: Bandit algorithm instance\n",
    "        X: Feature matrix\n",
    "        arms: True arm assignments\n",
    "        rewards: True rewards\n",
    "        context_cols: Context columns (optional)\n",
    "        cohort_col: Cohort column (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (rewards_list, selected_arms)\n",
    "    \"\"\"\n",
    "    rewards_list = []\n",
    "    selected_arms = []\n",
    "    for i in range(0, len(X), algo.batch_size):\n",
    "        batch_X = X[i:i + algo.batch_size]\n",
    "        batch_arms = arms[i:i + algo.batch_size]\n",
    "        batch_rewards = rewards[i:i + algo.batch_size]\n",
    "        if isinstance(algo, BatchLinUCB):\n",
    "            chosen_arms = algo.select_arm(batch_X)\n",
    "            rewards_batch = [r if a == b else 0 for a, b, r in zip(chosen_arms, batch_arms, batch_rewards)]\n",
    "            algo.update_batch(np.array(chosen_arms), batch_X, np.array(rewards_batch))\n",
    "            rewards_list.extend(rewards_batch)\n",
    "            selected_arms.extend(chosen_arms)\n",
    "        elif isinstance(algo, BatchCohortThompsonSampling):\n",
    "            if cohort_col is not None and isinstance(cohort_col, (np.ndarray, list, pd.Series)):\n",
    "                cohort_batch = cohort_col[i:i + algo.batch_size]\n",
    "            else:\n",
    "                cohort_batch = batch_X[:, 0]\n",
    "            chosen_arms = algo.select_arm_batch(cohort_batch)\n",
    "            rewards_batch = [r if a == b else 0 for a, b, r in zip(chosen_arms, batch_arms, batch_rewards)]\n",
    "            algo.update_batch(cohort_batch, chosen_arms, rewards_batch)\n",
    "            rewards_list.extend(rewards_batch)\n",
    "            selected_arms.extend(chosen_arms)\n",
    "        else:\n",
    "            chosen_arms = algo.select_batch()\n",
    "            rewards_batch = [r if a == b else 0 for a, b, r in zip(chosen_arms, batch_arms, batch_rewards)]\n",
    "            algo.update_batch(chosen_arms, rewards_batch)\n",
    "            rewards_list.extend(rewards_batch)\n",
    "            selected_arms.extend(chosen_arms)\n",
    "    return np.array(rewards_list), np.array(selected_arms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9de137",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 2. Data Generation\n",
    "\n",
    "We'll generate synthetic data for our bandit experiments. The data will include:\n",
    "- One arm-specific feature for arm_1\n",
    "- Different base reward rates for each arm\n",
    "- Feature effects for arm_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd18991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data generation configuration\n",
    "n_samples = 10000\n",
    "n_arms = 4\n",
    "n_features = 0  # No general features\n",
    "n_informative = 0\n",
    "n_redundant = 0\n",
    "n_repeated = 0\n",
    "arm_effects = {\n",
    "    'arm_0': 0.0,    # Control arm\n",
    "    'arm_1': -0.01,  # Small negative effect\n",
    "    'arm_2': -0.02,  # Medium negative effect\n",
    "    'arm_3': -0.04   # Large negative effect\n",
    "}\n",
    "positive_class_proportion = 0.1\n",
    "random_seed = 20200103\n",
    "feature_association_list = [\"linear\"]\n",
    "random_select_association = False\n",
    "error_std = 0.05\n",
    "n_arm_features = {'arm_1': 1}  # Only arm_1 has a specific feature\n",
    "n_mixed_features = {'arm_1': 0, 'arm_2': 0, 'arm_3': 0}\n",
    "custom_coef_arm = True\n",
    "coef_arm_dict = {'arm_1': [0.5]}  # One coefficient for arm_1's feature\n",
    "custom_coef_informative = True\n",
    "coef_informative_list = [1.2, 1.0, 1.1, 0.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2f013",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Generate Feature Matrix and Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9619a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix\n",
    "X = np.zeros((n_samples, 1))  # Only one feature for arm_1\n",
    "X[:, 0] = np.random.normal(0, 1, n_samples)  # Random feature values\n",
    "\n",
    "# Generate arm assignments and rewards\n",
    "arms = np.random.choice(['arm_0', 'arm_1', 'arm_2', 'arm_3'], n_samples)\n",
    "rewards = np.zeros(n_samples)\n",
    "\n",
    "# Set base reward rates for each arm\n",
    "base_rates = {\n",
    "    'arm_0': 0.1,\n",
    "    'arm_1': 0.1,\n",
    "    'arm_2': 0.08,\n",
    "    'arm_3': 0.06\n",
    "}\n",
    "\n",
    "# Generate rewards\n",
    "for i in range(n_samples):\n",
    "    arm = arms[i]\n",
    "    if arm == 'arm_1':\n",
    "        # Add feature effect for arm_1\n",
    "        feature_effect = np.dot(X[i], coef_arm_dict['arm_1'])\n",
    "        reward_prob = base_rates[arm] + feature_effect\n",
    "    else:\n",
    "        reward_prob = base_rates[arm]\n",
    "    reward_prob = np.clip(reward_prob, 0, 1)  # Ensure valid probability\n",
    "    rewards[i] = np.random.binomial(1, reward_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5fe036",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Data Statistics and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdab0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print data generation statistics\n",
    "print(\"\\nData Generation Statistics:\")\n",
    "print(f\"Total samples: {n_samples}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(\"\\nArm reward rates:\")\n",
    "for arm in np.unique(arms):\n",
    "    rate = np.mean(rewards[arms == arm])\n",
    "    print(f\"{arm}: {rate:.2%}\")\n",
    "\n",
    "# Create feature name mapping for plots\n",
    "feature_map = {f\"feature_{i}\": f\"Feature {i}\" for i in range(X.shape[1])}\n",
    "\n",
    "# Plot reward by feature value and arm (bar chart grid)\n",
    "n_bins = 10\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(X.shape[1] / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 2.5 * n_rows), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    feature = f\"feature_{i}\"\n",
    "    df = pd.DataFrame({feature: X[:, i]})\n",
    "    df['arm'] = arms\n",
    "    df['reward'] = rewards\n",
    "    df[f'{feature}_binned'] = pd.qcut(df[feature], q=n_bins, duplicates='drop').cat.codes + 1\n",
    "    grouped = df.groupby([f'{feature}_binned', 'arm'])['reward'].mean().unstack()\n",
    "    grouped.plot(kind='bar', ax=axes[i], width=0.8)\n",
    "    axes[i].set_xlabel('Feature Value Bin', fontsize=10)\n",
    "    axes[i].set_ylabel('Reward', fontsize=10)\n",
    "    axes[i].set_title(f'{feature_map[feature]}', fontsize=12)\n",
    "    axes[i].set_ylim([0, 0.25])\n",
    "    if i == X.shape[1] - 1:\n",
    "        axes[i].legend(title='Arms', loc='center left', bbox_to_anchor=(1.0, 0.5), fontsize=10, title_fontsize=10)\n",
    "    else:\n",
    "        axes[i].legend().set_visible(False)\n",
    "    axes[i].tick_params(axis='x', rotation=0)\n",
    "    for spine in axes[i].spines.values():\n",
    "        spine.set_edgecolor('black')\n",
    "        spine.set_linewidth(1.5)\n",
    "    df.drop(columns=[f'{feature}_binned'], inplace=True)\n",
    "\n",
    "# Remove any empty subplots\n",
    "for j in range(i+1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print feature type counts\n",
    "print(\"\\nFeature types:\")\n",
    "print(f\"Informative features: {n_informative}\")\n",
    "print(f\"Arm-specific features: {sum(n_arm_features.values())}\")\n",
    "print(f\"Mixed features: {sum(n_mixed_features.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee8fa6b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Prepare Cohort Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f530e321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cohort information for cohort-based algorithms\n",
    "df = pd.DataFrame({\n",
    "    'arm': arms,\n",
    "    'reward': rewards,\n",
    "    'feature_0': X[:, 0]\n",
    "})\n",
    "df['cohort'] = np.random.choice([0, 1, 2], size=len(df))\n",
    "\n",
    "# Show the first few rows\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8587bca3",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 3. Initialize and Run Bandit Algorithms\n",
    "\n",
    "We'll demonstrate both classical and contextual bandits. All bandits are fit using the consistent API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16892600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature columns for contextual bandits\n",
    "feature_cols = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "\n",
    "# Initialize and fit bandits to set up their internal state\n",
    "# Classical bandits\n",
    "eg = BatchBandit(EpsilonGreedy(epsilon=0.3), batch_size=100)\n",
    "eg.fit(arms, rewards)\n",
    "\n",
    "ucb = BatchBandit(UCB(alpha=1.0), batch_size=100)\n",
    "ucb.fit(arms, rewards)\n",
    "\n",
    "ts = BatchBandit(ThompsonSampling(), batch_size=100)\n",
    "ts.fit(arms, rewards)\n",
    "\n",
    "# Contextual bandits\n",
    "linucb = BatchLinUCB(alpha=1.0, batch_size=100)\n",
    "linucb.fit(X, arms, rewards)\n",
    "\n",
    "# Bin the feature into 10 bins using pd.qcut\n",
    "n_bins_mab = 10\n",
    "feature_for_cohort = 0  # Only one feature, index 0\n",
    "cohort_bins = pd.qcut(X[:, feature_for_cohort], n_bins_mab, labels=False, duplicates=\"drop\")\n",
    "\n",
    "# Use the binned cohort in the bandit\n",
    "cohort_ts = BatchCohortThompsonSampling(batch_size=100)\n",
    "cohort_ts.fit(X, arms, rewards, cohort_feature=cohort_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2586e996",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Run Online Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32568bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results['BatchEpsilonGreedy'] = run_online_bandit(eg, X, arms, rewards)\n",
    "results['BatchUCB'] = run_online_bandit(ucb, X, arms, rewards)\n",
    "results['BatchThompsonSampling'] = run_online_bandit(ts, X, arms, rewards)\n",
    "results['BatchLinUCB'] = run_online_bandit(linucb, X, arms, rewards)\n",
    "results['BatchCohortThompsonSampling'] = run_online_bandit(cohort_ts, X, arms, rewards, cohort_col=cohort_bins)\n",
    "\n",
    "# Print first 10 rewards and selected arms for each algorithm\n",
    "for name, (rewards, selected_arms) in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"First 10 rewards: {rewards[:10]}\")\n",
    "    print(f\"First 10 selected arms: {selected_arms[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c3fee6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 4. Evaluate and Visualize Results\n",
    "\n",
    "We'll create visualizations for:\n",
    "1. Cumulative reward over time\n",
    "2. Cumulative regret over time\n",
    "3. Arm selection frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15484711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative reward\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, (rewards, _) in results.items():\n",
    "    cum_reward = cumulative_reward(rewards)\n",
    "    plt.plot(cum_reward, label=name)\n",
    "plt.title('Cumulative Reward Over Time')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot cumulative regret\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Calculate optimal reward based on our data generation configuration\n",
    "# arm_0 has the highest base rate of 0.1\n",
    "optimal_reward = 0.1  # This is the base rate of arm_0\n",
    "for name, (rewards, _) in results.items():\n",
    "    cum_regret = cumulative_regret(rewards, optimal_reward=optimal_reward)\n",
    "    plt.plot(cum_regret, label=name)\n",
    "plt.title('Cumulative Regret Over Time')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Cumulative Regret')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot arm selection frequency\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, (_, selected_arms) in results.items():\n",
    "    arm_freq = pd.Series(selected_arms).value_counts(normalize=True)\n",
    "    plt.bar(arm_freq.index, arm_freq.values, alpha=0.5, label=name)\n",
    "plt.title('Arm Selection Frequency')\n",
    "plt.xlabel('Arm')\n",
    "plt.ylabel('Selection Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fb6922",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 5. Summary\n",
    "\n",
    "In this notebook, we demonstrated:\n",
    "1. How to generate synthetic bandit data with arm-specific features\n",
    "2. How to initialize and run different bandit algorithms\n",
    "3. How to evaluate and visualize the performance of each algorithm\n",
    "\n",
    "The results show how different algorithms perform in terms of:\n",
    "- Cumulative reward\n",
    "- Cumulative regret\n",
    "- Arm selection frequency\n",
    "\n",
    "You can modify the parameters and configurations to experiment with different scenarios."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
