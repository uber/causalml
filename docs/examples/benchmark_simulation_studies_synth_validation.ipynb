{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Learner Benchmarks with Semi-synthetic Data in Schuler, A., Jung, K., Tibshirani, R., Hastie, T., and Shah, N. Synth-validation: Selecting the best causal inference method for a given dataset (2017)\n",
    "\n",
    "This notebook compares X-, R-, and T learners using the Constrained gradient boosting semi synthetic framework described in  [Schuler, A., Jung, K., Tibshirani, R., Hastie, T., and Shah, N (2017)](https://arxiv.org/pdf/1711.00083) using the IHDP dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from causalml.inference.meta import BaseTRegressor\n",
    "from causalml.inference.meta import BaseXRegressor\n",
    "from causalml.inference.meta import BaseRRegressor\n",
    "\n",
    "from causalml.dataset.synthValidation import SynthDataGenerator\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "print(importlib.metadata.version('causalml') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/ihdp_npci_8.csv', header=None)\n",
    "cols =  [\"treatment\", \"y_factual\", \"y_cfactual\", \"mu0\", \"mu1\"] + [str(i) for i in range(25)]\n",
    "data.columns = cols\n",
    "\n",
    "X = data[[str(i) for i in range(5)]]\n",
    "y = data[\"y_factual\"]\n",
    "w = data[\"treatment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveLearner():\n",
    "    def _init_(self):\n",
    "        pass\n",
    "    def fit(self, X, treatment, y):\n",
    "        self.ate = y[treatment==1].mean() - y[treatment==0].mean()\n",
    "    def predict(self, X, p):\n",
    "        return np.repeat(self.ate, len(X))  \n",
    "    def estimate_ate(self, X, treatment, y):\n",
    "        ate = y[treatment==1].mean() - y[treatment==0].mean()\n",
    "        return [ate] # No need for CI right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(X, w, y, learner_dict, propensity_learner, K=10, n=None, **data_generator_kwargs):\n",
    "    \n",
    "    synth_gen = SynthDataGenerator(**data_generator_kwargs)\n",
    "    synth_gen.fit(X, w, y)   \n",
    "    datasets = synth_gen.generate(K=K, n=n)\n",
    "    result_list = []\n",
    "\n",
    "    for q in range(len(datasets)):\n",
    "        for k in range(len(datasets[q])):\n",
    "            X = datasets[q][k][[str(i) for i in range(5)]]\n",
    "            w = datasets[q][k]['w']\n",
    "            y = datasets[q][k]['y']\n",
    "            tau_i = datasets[q][k]['tau_i']\n",
    "            X_train, X_test, w_train, _, y_train, _, _, tau_test = train_test_split(\n",
    "                X, w, y, tau_i, test_size=0.2, random_state=111)\n",
    "\n",
    "            em = clone(propensity_learner)\n",
    "            em.fit(X_train, w_train)\n",
    "            e_hat_test = em.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            for learner in learner_dict.keys():\n",
    "                model = deepcopy(learner_dict[learner])\n",
    "                model.fit(X = X_train, treatment = w_train, y = y_train)\n",
    "                hat_tau = model.predict(X_test, p=e_hat_test)\n",
    "                pehe = mean_squared_error(tau_test, hat_tau)\n",
    "                result_list.append([q, k, learner, pehe])\n",
    "    \n",
    "    cols = ['q', 'k', 'learner', 'pehe']\n",
    "    df_res = pd.DataFrame(result_list, columns=cols)\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso based experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_dict = {\n",
    "    'Naive-Learner': NaiveLearner(),\n",
    "    'T-Learner': BaseTRegressor(learner=Lasso()),\n",
    "    'X-Learner': BaseXRegressor(learner=Lasso()),\n",
    "    'R-Learner': BaseRRegressor(learner=Lasso())\n",
    "}\n",
    "\n",
    "propensity_learner = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "df_res_lasso = run_experiments(X, w, y, learner_dict, propensity_learner, Q = 5, B=1, n = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='learner', y='pehe', data=df_res_lasso, linewidth=1, showfliers=False)\n",
    "plt.ylabel('PEHE (MSE)')\n",
    "plt.xlabel('')\n",
    "plt.title('All experiments (Lasso)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper discusses benchmarking ATE esimation so let's do that as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments2(X, w, y, learner_dict, K=10, n = None, **data_generator_kwargs):\n",
    "    \n",
    "    synth_gen = SynthDataGenerator(**data_generator_kwargs)\n",
    "    synth_gen.fit(X, w, y)   \n",
    "    datasets = synth_gen.generate(K=K, n=n)\n",
    "    result_list = []\n",
    "\n",
    "    for q in range(len(datasets)):\n",
    "        for k in range(len(datasets[q])):\n",
    "            X = datasets[q][k][[str(i) for i in range(5)]]\n",
    "            w = datasets[q][k]['w']\n",
    "            y = datasets[q][k]['y']\n",
    "            tau_i = datasets[q][k]['tau_i']\n",
    "            \n",
    "            X_train, X_test, w_train, _, y_train, _, _, tau_test = train_test_split(\n",
    "                X, w, y, tau_i, test_size=0.2, random_state=111)\n",
    "\n",
    "            true_ate = tau_test.mean()\n",
    "\n",
    "            for learner in learner_dict.keys():\n",
    "                model = deepcopy(learner_dict[learner])\n",
    "                ate_hat = float(model.estimate_ate(X = X_train, treatment = w_train, y = y_train)[0])\n",
    "                ate_diff = np.abs(ate_hat - true_ate)\n",
    "                result_list.append([q, k, learner, ate_diff, true_ate])\n",
    "    \n",
    "    cols = ['q', 'k', 'learner', 'ate_diff', 'true_ate']\n",
    "    df_res = pd.DataFrame(result_list, columns=cols)\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_dict = {\n",
    "    'Naive-Learner': NaiveLearner(),\n",
    "    'T-Learner': BaseTRegressor(learner=Lasso()),\n",
    "    'X-Learner': BaseXRegressor(learner=Lasso()),\n",
    "    'R-Learner': BaseRRegressor(learner=Lasso())\n",
    "}\n",
    "\n",
    "df_res_lasso2 = run_experiments2(X, w, y, learner_dict, Q = 5, B=1, n = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='learner', y='ate_diff', data=df_res_lasso2, linewidth=1, showfliers=False)\n",
    "plt.ylabel('ATE absolute diff')\n",
    "plt.xlabel('')\n",
    "plt.title('All experiments (Lasso)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
