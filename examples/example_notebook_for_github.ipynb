{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `causalml` - Meta-Learner Example Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "CausalML is a Python package that provides a suite of uplift modeling and causal inference methods using machine learning algorithms based on recent research. It provides a standard interface that allows user to estimate the Conditional Average Treatment Effect (CATE) or Individual Treatment Effect (ITE) from experimental or observational data. Essentially, it estimates the causal impact of intervention T on outcome Y for users with observed features X, without strong assumptions on the model form.  The package currently supports the following methods:\n",
    "- Tree-based algorithms\n",
    "    - Uplift tree/random forests on KL divergence, Euclidean Distance, and Chi-Square\n",
    "    - Uplift tree/random forests on Contextual Treatment Selection\n",
    "- Meta-learner algorithms\n",
    "    - S-learner\n",
    "    - T-learner\n",
    "    - X-learner\n",
    "    - R-learner\n",
    "    \n",
    "In this notebook, we will generate some synthetic data to demonstrate how to use the various Meta-Learner algorithms in order to estimate Individual Treatment Effects (and Average Treatment Effects with confidence intervals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "\n",
    "from causalml.inference.meta import LRSLearner\n",
    "from causalml.inference.meta import XGBTLearner, MLPTLearner\n",
    "from causalml.inference.meta import BaseXLearner, BaseRLearner, BaseSLearner, BaseTLearner\n",
    "from causalml.match import NearestNeighborMatch, create_table_one\n",
    "from causalml.propensity import ElasticNetPropensityModel\n",
    "from causalml.dataset import *\n",
    "from causalml.features import OUTCOME_COL, TREATMENT_COL, SCORE_COL, INFERENCE_FEATURES\n",
    "from causalml.features import MATCHING_COVARIATES, PROPENSITY_FEATURES, PROPENSITY_FEATURE_TRANSFORMATIONS\n",
    "from causalml.features import INFERENCE_FEATURE_TRANSFORMATIONS, load_data, INFERENCE_FEATURES\n",
    "from causalml.metrics import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generate synthetic data\n",
    "- We have implemented 4 modes of generating synthetic data (specified by input parameter `mode`). Refer to the References section for more detail on these data generation processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data using mode 1\n",
    "y, X, treatment, tau, b, e = synthetic_data(mode=1, n=1000, p=5, sigma=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calculate Propensity Scores\n",
    "Although we have propensity scores (`e`) generated from the `synthetic_data` function, typically in reality we aren't able to directly observe these values (unless a separate model has been independently developed for the treatment flag, in which case we could use that). We have developed a light-weight propensity model that allows you to specify the features to estimate treatment-propensity from.\n",
    "\n",
    "*Note that propensity scores are only used for X Learner and R Learner.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict p_hat because e would not be directly observed in real-life\n",
    "p_model = ElasticNetPropensityModel()\n",
    "p_hat = p_model.fit_predict(X, treatment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate Average Treatment Effect (ATE)\n",
    "A meta-learner can be instantiated by calling a base learner class and providing an sklearn/xgboost regressor class as input. Alternatively, we have provided some ready-to-use learners that have already inherited their respective base learner class capabilities. This is more abstracted and allows these tools to be quickly and readily usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5884572436573214, 0.4385237264356304, 0.7383907608790125)\n",
      "ATE estimate: 0.588\n",
      "ATE lower bound: 0.439\n",
      "ATE upper bound: 0.738\n"
     ]
    }
   ],
   "source": [
    "# Ready-to-use S-Learner using LinearRegression\n",
    "learner_s = LRSLearner()\n",
    "ate_s = learner_s.estimate_ate(X, treatment, y)\n",
    "print(ate_s)\n",
    "print('ATE estimate: {:.03f}'.format(ate_s[0]))\n",
    "print('ATE lower bound: {:.03f}'.format(ate_s[1]))\n",
    "print('ATE upper bound: {:.03f}'.format(ate_s[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5'] ['f0', 'f1', 'f2', 'f3', 'f4']\nexpected f5 in input data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-532093ab47e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_ate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreatment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda2/envs/python3.6/lib/python3.6/site-packages/causalml/inference/meta/tlearner.py\u001b[0m in \u001b[0;36mestimate_ate\u001b[0;34m(self, X, treatment, y)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_treatment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0myhat_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0myhat_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/python3.6/lib/python3.6/site-packages/xgboost-0.90-py3.6.egg/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, validate_features)\u001b[0m\n\u001b[1;32m    464\u001b[0m                                           \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                                           \u001b[0mntree_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mntree_limit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                                           validate_features=validate_features)\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/python3.6/lib/python3.6/site-packages/xgboost-0.90-py3.6.egg/xgboost/core.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features)\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/python3.6/lib/python3.6/site-packages/xgboost-0.90-py3.6.egg/xgboost/core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m                 raise ValueError(msg.format(self.feature_names,\n\u001b[0;32m-> 1695\u001b[0;31m                                             data.feature_names))\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_split_value_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_pandas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5'] ['f0', 'f1', 'f2', 'f3', 'f4']\nexpected f5 in input data"
     ]
    }
   ],
   "source": [
    "learner_t.estimate_ate(X, treatment, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5'] ['f0', 'f1', 'f2', 'f3', 'f4']\nexpected f5 in input data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-7e653eba5707>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Ready-to-use T-Learner using XG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlearner_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBTLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mate_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_ate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreatment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mate_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/python3.6/lib/python3.6/site-packages/causalml/inference/meta/tlearner.py\u001b[0m in \u001b[0;36mestimate_ate\u001b[0;34m(self, X, treatment, y)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_treatment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0myhat_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0myhat_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/python3.6/lib/python3.6/site-packages/xgboost-0.90-py3.6.egg/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, validate_features)\u001b[0m\n\u001b[1;32m    464\u001b[0m                                           \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                                           \u001b[0mntree_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mntree_limit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                                           validate_features=validate_features)\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/python3.6/lib/python3.6/site-packages/xgboost-0.90-py3.6.egg/xgboost/core.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features)\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/python3.6/lib/python3.6/site-packages/xgboost-0.90-py3.6.egg/xgboost/core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m                 raise ValueError(msg.format(self.feature_names,\n\u001b[0;32m-> 1695\u001b[0;31m                                             data.feature_names))\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_split_value_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_pandas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5'] ['f0', 'f1', 'f2', 'f3', 'f4']\nexpected f5 in input data"
     ]
    }
   ],
   "source": [
    "# Ready-to-use T-Learner using XGB\n",
    "learner_t = XGBTLearner()\n",
    "ate_t = learner_t.estimate_ate(X, treatment, y)\n",
    "print(ate_s)\n",
    "\n",
    "# Calling the Base Learner class and feeding in a specified model\n",
    "learner_t = BaseTLearner(XGBRegressor())\n",
    "ate_t = learner_t.estimate_ate(X, treatment, y)\n",
    "print(ate_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for base_learner,label_l in zip([BaseSLearner, BaseTLearner, BaseXLearner, BaseRLearner],['S', 'T', 'X', 'R']):\n",
    "        for model,label_m in zip([LinearRegression, XGBRegressor],['LR', 'XGB']):\n",
    "            learner = base_learner(model())\n",
    "            try:\n",
    "                preds_dict['{} Learner ({})'.format(label_l, label_m)] = learner.fit_predict(X=X, p=p_hat, treatment=w, y=y)\n",
    "            except TypeError:\n",
    "                preds_dict['{} Learner ({})'.format(label_l, label_m)] = learner.fit_predict(X=X, treatment=w, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the Meta-Learners' Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size=0.2\n",
    "X_tr, X_val, y_tr, y_val, treatment_tr, treatment_val, tau_tr, tau_val, b_tr, b_val, e_tr, e_val = \\\n",
    "    train_test_split(X, y, treatment, tau, b, e, test_size=valid_size, random_state=123, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "-  Split the synthetic data set 20% for validation and 80% for training, X Learner (XGB) and R Learner (XGB) are still the best performing meta-learners (using MSE, Absolute % Error from ATE, and KL Divergence as measures of training and validaiton performance)\n",
    "-  For synthetic data Method 2 (randomized trial) and Method 4 (unrelated treatment and control groups), when the sample size is small (8k for training and 2k for validation) and only 10 simulaitions, training performance of all three metrics better than validation; as increasing the sample size (40k for training and 10k for validation) and also with more 100 simulations, results are becoming very close for training and validation\n",
    "-  For synthetic data Method 1 and Method 3, training and validation already have similar results even with smaller \n",
    "sample size.\n",
    "-  Looking at AUUC values of cumulative gains of model estimates, training and validaiton results are still consistent\n",
    "\n",
    "# Methodology\n",
    "Using the methods outlined ['Quasi-Oracle Estimation of Heterogeneous Treatment Effects' (Nie X. and Wager S., 2018)](https://arxiv.org/pdf/1712.04912.pdf), we compare the S/T/X/R Meta Learners using a linear model (sklearn) and a boosted tree model (xgboost) to understand which learners and models perform well in different scenarios. \n",
    "\n",
    "- Method 1: synthetic data with a diffult nuisance components and an easy treatment effect\n",
    "- Method 2: synthetic data of a randomized trial\n",
    "- Method 3: synthetic data with easy propensity and a difficult baseline\n",
    "- Method 4: synthetic data with unrelated treatment and control groups\n",
    "\n",
    "For each method, we run `k` simulations, where each simulation generates `n` samples and split 20% as the hold-out data set for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:47:58.174166Z",
     "start_time": "2019-05-15T22:47:58.118948Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get hte synthetic data\n",
    "preds_dict_train, preds_dict_valid = get_synthetic_preds_holdout(simulate_nuisance_and_easy_treatment, n=10000, valid_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check if split the data correctly\n",
    "print(preds_dict_train['generated_data']['y'].shape, \n",
    "      preds_dict_valid['generated_data']['y'].shape\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_summary_1,validation_summary_1  = get_synthetic_summary_holdout(simulate_nuisance_and_easy_treatment, n=10000, valid_size=0.2, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_summary_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_summary_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scatter_plot_summary_holdout(train_summary_1, validation_summary_1, k=10, label=['Train', 'Validaiton'], drop_learners=[], drop_cols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time train_summary_1_50k_10, validation_summary_1_50k_10  = get_synthetic_summary_holdout(simulate_nuisance_and_easy_treatment, n=50000, valid_size=0.2, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with 50k samples and 10 simulations\n",
    "scatter_plot_summary_holdout(train_summary_1_50k_10, validation_summary_1_50k_10, k=10, label=['Train', 'Validaiton'], drop_learners=[], drop_cols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time train_summary_1_50k, validation_summary_1_50k  = get_synthetic_summary_holdout(simulate_nuisance_and_easy_treatment, n=50000, valid_size=0.2, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with 50k samples and 100 simulations\n",
    "scatter_plot_summary_holdout(train_summary_1_50k, validation_summary_1_50k, k=100, label=['Train', 'Validaiton'], drop_learners=[], drop_cols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bar_plot_summary_holdout(train_summary_1, validation_summary_1, k=10, drop_learners=['S Learner (LR)'], drop_cols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Single simulation (50k samples)\n",
    "synthetic_preds_holdout_train_1, synthetic_preds_holdout_valid_1 = get_synthetic_preds_holdout(simulate_nuisance_and_easy_treatment, n=50000, valid_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:43:25.011911Z",
     "start_time": "2019-05-15T22:43:20.166860Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#distribution plot for signle simulation of Training\n",
    "distr_plot_single_sim(synthetic_preds_holdout_train_1, kind='kde', linewidth=2, bw_method=0.5,\n",
    "           drop_learners=['S Learner (LR)',' S Learner (XGB)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#distribution plot for signle simulation of Validaiton\n",
    "distr_plot_single_sim(synthetic_preds_holdout_valid_1, kind='kde', linewidth=2, bw_method=0.5,\n",
    "           drop_learners=['S Learner (LR)', 'S Learner (XGB)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:43:30.804031Z",
     "start_time": "2019-05-15T22:43:25.014376Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scatter Plots for a Single Simulation of Training Data\n",
    "scatter_plot_single_sim(synthetic_preds_holdout_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scatter Plots for a Single Simulation of Validaiton Data\n",
    "scatter_plot_single_sim(synthetic_preds_holdout_valid_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cumulitive Gain AUUC values for a Single Simulation of Training Data\n",
    "get_synthetic_auuc(synthetic_preds_holdout_train_1, drop_learners=['S Learner (LR)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cumulitive Gain AUUC values for a Single Simulation of Validaiton Data\n",
    "get_synthetic_auuc(synthetic_preds_holdout_valid_1, drop_learners=['S Learner (LR)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time train_summary_2, validation_summary_2  = get_synthetic_summary_holdout(simulate_randomized_trial, n=10000, valid_size=0.2, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_summary_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_summary_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scatter_plot_summary_holdout(train_summary_2, validation_summary_2, k=10, label=['Train','Validaiton'], drop_learners=['S Learner (LR)'], drop_cols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with 50k samples and 10 simulations\n",
    "%time train_summary_2_50k_10, validation_summary_2_50k_10  = get_synthetic_summary_holdout(simulate_randomized_trial, n=50000, valid_size=0.2, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scatter_plot_summary_holdout(train_summary_2_50k_10, validation_summary_2_50k_10, k=10, label=['Train', 'Validaiton'], drop_learners=['S Learner (LR)'], drop_cols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with 50k samples and 100 simulations\n",
    "%time train_summary_2_50k_100, validation_summary_2_50k_100  = get_synthetic_summary_holdout(simulate_randomized_trial, n=50000, valid_size=0.2, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scatter_plot_summary_holdout(train_summary_2_50k_100, validation_summary_2_50k_100, k=100, label=['Train', 'Validaiton'], drop_learners=['S Learner (LR)'], drop_cols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bar_plot_summary_holdout(train_summary_2, validation_summary_2, k=10, drop_learners=['S Learner (LR)','S Learner (XGB)'], drop_cols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Single simulation (50k samples)\n",
    "synthetic_preds_holdout_train_2, synthetic_preds_holdout_valid_2 = get_synthetic_preds_holdout(simulate_randomized_trial, n=50000, valid_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:43:25.011911Z",
     "start_time": "2019-05-15T22:43:20.166860Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#distribution plot for signle simulation of Training\n",
    "distr_plot_single_sim(synthetic_preds_holdout_train_2, kind='kde', linewidth=2, bw_method=0.5, \n",
    "           drop_learners=['S Learner (LR)', 'S Learner (XGB)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#distribution plot for signle simulation of Validaiton\n",
    "distr_plot_single_sim(synthetic_preds_holdout_valid_2, kind='kde', linewidth=2, bw_method=0.5, \n",
    "           drop_learners=['S Learner (LR)', 'S Learner (XGB)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:43:30.804031Z",
     "start_time": "2019-05-15T22:43:25.014376Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scatter Plots for a Single Simulation for Training Data\n",
    "scatter_plot_single_sim(synthetic_preds_holdout_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scatter Plots for a Single Simulation for Validaiton Data\n",
    "scatter_plot_single_sim(synthetic_preds_holdout_valid_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cumulitive Gain AUuC values for a Single Simulation of Training Data\n",
    "get_synthetic_auuc(synthetic_preds_holdout_train_2, drop_learners=['S Learner (LR)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cumulitive Gain AUUC values for a Single Simulation of Validaiton Data\n",
    "get_synthetic_auuc(synthetic_preds_holdout_valid_2, drop_learners=['S Learner (LR)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:44:57.994879Z",
     "start_time": "2019-05-15T22:44:28.062360Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "train_summary_3, validation_summary_3  = get_synthetic_summary_holdout(simulate_easy_propensity_difficult_baseline, n=10000, valid_size=0.2, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_summary_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_summary_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scatter_plot_summary_holdout(train_summary_3, validation_summary_3, k=10, label=['Train', 'Validaiton'], drop_learners=['X Learner (LR)', 'T Learner (LR)'], drop_cols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with 50k samples and 10 simulations\n",
    "%time train_summary_3_50k_10, validation_summary_3_50k_10  = get_synthetic_summary_holdout(simulate_easy_propensity_difficult_baseline, n=50000, valid_size=0.2, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scatter_plot_summary_holdout(train_summary_3_50k_10, validation_summary_3_50k_10, k=10, label=['Train', 'Validaiton'], drop_learners=['X Learner (LR)', 'T Learner (LR)'], drop_cols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with 50k samples and 100 simulations\n",
    "%time train_summary_3_50k_100, validation_summary_3_50k_100  = get_synthetic_summary_holdout(simulate_easy_propensity_difficult_baseline, n=50000, valid_size=0.2, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scatter_plot_summary_holdout(train_summary_3_50k_100, validation_summary_3_50k_100, k=10, label=['Train', 'Validaiton'], drop_learners=['X Learner (LR)', 'T Learner (LR)'], drop_cols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bar_plot_summary_holdout(train_summary_3, validation_summary_3, k=10, drop_learners=[], drop_cols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Single simulation (50k samples)\n",
    "synthetic_preds_holdout_train_3, synthetic_preds_holdout_valid_3 = get_synthetic_preds_holdout(simulate_easy_propensity_difficult_baseline, n=50000, valid_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`distr_plot()` and `scatter_plot_predictions()` not applicable for Method 3, since Actuals are uniform values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time train_summary_4, validation_summary_4 = get_synthetic_summary_holdout(simulate_unrelated_treatment_control, n=10000, valid_size=0.2, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_summary_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_summary_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time train_summary_4_50k, validation_summary_4_50k  = get_synthetic_summary_holdout(simulate_unrelated_treatment_control, n=50000, valid_size=0.2, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time train_summary_4_50k_100, validation_summary_4_50k_100  = get_synthetic_summary_holdout(simulate_unrelated_treatment_control, n=50000, valid_size=0.2, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with 10k samples and 10 simulations\n",
    "scatter_plot_summary_holdout(train_summary_4, validation_summary_4, k=10, label=['Train', 'Validaiton'], drop_learners=['S Learner (LR)'], drop_cols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with 50k samples and 10 simulations\n",
    "scatter_plot_summary_holdout(train_summary_4_50k, validation_summary_4_50k, k=10, label=['Train', 'Validaiton'], drop_learners=['S Learner (LR)'], drop_cols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with 50k samples and 100 simulations\n",
    "scatter_plot_summary_holdout(train_summary_4_50k_100, validation_summary_4_50k_100, k=100, label=['Train', 'Validaiton'], drop_learners=['S Learner (LR)'], drop_cols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bar_plot_summary_holdout(train_summary_4, validation_summary_4, k=10, drop_learners=['S Learner (LR)'], drop_cols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Single simulation (50k samples)\n",
    "synthetic_preds_holdout_train_4, synthetic_preds_holdout_valid_4 = get_synthetic_preds_holdout(simulate_unrelated_treatment_control, n=50000, valid_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:43:25.011911Z",
     "start_time": "2019-05-15T22:43:20.166860Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#distribution plot for signle simulation of Training\n",
    "distr_plot_single_sim(synthetic_preds_holdout_train_4, kind='kde', linewidth=2, bw_method=0.5, \n",
    "           drop_learners=['S Learner (LR)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#distribution plot for signle simulation of Validaiton\n",
    "distr_plot_single_sim(synthetic_preds_holdout_valid_4, kind='kde', linewidth=2, bw_method=0.5, \n",
    "           drop_learners=['S Learner (LR)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:43:30.804031Z",
     "start_time": "2019-05-15T22:43:25.014376Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scatter Plots for a Single Simulation for Training\n",
    "scatter_plot_single_sim(synthetic_preds_holdout_train_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scatter Plots for a Single Simulation for Validaiton\n",
    "scatter_plot_single_sim(synthetic_preds_holdout_valid_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cumulitive Gain AUUC values for a Single Simulation of Training Data\n",
    "get_synthetic_auuc(synthetic_preds_holdout_train_4, drop_learners=['S Learner (LR)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cumulitive Gain AUUC values for a Single Simulation of Validaiton Data\n",
    "get_synthetic_auuc(synthetic_preds_holdout_valid_4, drop_learners=['S Learner (LR)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO REMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synthetic_preds_holdout(synthetic_data_func, n=1000, valid_size = 0.2):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \"\"\"Generate predictions for synthetic data using specified function (single simulation) for train and holdout\n",
    "\n",
    "    Args:\n",
    "        synthetic_data_func (function): synthetic data generation function\n",
    "        n (int, optional): number of samples\n",
    "        valid_size(float,optional): validaiton/hold out data size\n",
    "\n",
    "    Returns:\n",
    "        (tuple): synthetic training and validation data dictionaries:\n",
    "\n",
    "          - preds_dict_train (dict): synthetic training data dictionary\n",
    "          - preds_dict_valid (dict): synthetic validation data dictionary\n",
    "    \"\"\"\n",
    "    y, X, w, tau, b, e = synthetic_data_func(n=n)\n",
    "\n",
    "    X_train, X_val, y_train, y_val, w_train, w_val, tau_train, tau_val, b_train, b_val, e_train, e_val = \\\n",
    "        train_test_split(X, y, w, tau, b, e, test_size=valid_size, random_state=40, shuffle=True)\n",
    "\n",
    "    preds_dict_train = {}\n",
    "    preds_dict_valid = {}\n",
    "\n",
    "    preds_dict_train[KEY_ACTUAL] = tau_train\n",
    "    preds_dict_valid[KEY_ACTUAL] = tau_val\n",
    "\n",
    "    preds_dict_train['generated_data'] = {\n",
    "        'y': y_train,\n",
    "        'X': X_train,\n",
    "        'w': w_train,\n",
    "        'tau': tau_train,\n",
    "        'b': b_train,\n",
    "        'e': e_train}\n",
    "    preds_dict_valid['generated_data'] = {\n",
    "        'y': y_val,\n",
    "        'X': X_val,\n",
    "        'w': w_val,\n",
    "        'tau': tau_val,\n",
    "        'b': b_val,\n",
    "        'e': e_val}\n",
    "\n",
    "    # Predict p_hat because e would not be directly observed in real-life\n",
    "    p_model = ElasticNetPropensityModel()\n",
    "    p_hat_train = p_model.fit_predict(X_train, w_train)\n",
    "    p_hat_val = p_model.fit_predict(X_val, w_val)\n",
    "\n",
    "    for base_learner, label_l in zip([BaseSLearner, BaseTLearner, BaseXLearner, BaseRLearner],['S', 'T', 'X', 'R']):\n",
    "        for model, label_m in zip([LinearRegression, XGBRegressor],['LR', 'XGB']):\n",
    "            ###RLearner will need to fit on the p_hat\n",
    "            if label_l != 'R':\n",
    "                learner = base_learner(model())\n",
    "                #fit the model on training data only\n",
    "                learner.fit(X=X_train, treatment=w_train, y=y_train)\n",
    "                try:\n",
    "                    preds_dict_train['{} Learner ({})'.format(\n",
    "                        label_l, label_m)] = learner.predict(X=X_train, p=p_hat_train).flatten()\n",
    "                    preds_dict_valid['{} Learner ({})'.format(\n",
    "                        label_l, label_m)] = learner.predict(X=X_val, p=p_hat_val).flatten()\n",
    "                except TypeError:\n",
    "                    preds_dict_train['{} Learner ({})'.format(\n",
    "                        label_l, label_m)] = learner.predict(X=X_train, treatment=w_train, y=y_train).flatten()\n",
    "                    preds_dict_valid['{} Learner ({})'.format(\n",
    "                        label_l, label_m)] = learner.predict(X=X_val, treatment=w_val, y=y_val).flatten()\n",
    "            else:\n",
    "                learner = base_learner(model())\n",
    "                learner.fit(X=X_train, p=p_hat_train, treatment=w_train, y=y_train)\n",
    "                preds_dict_train['{} Learner ({})'.format(\n",
    "                    label_l, label_m)] = learner.predict(X=X_train).flatten()\n",
    "                preds_dict_valid['{} Learner ({})'.format(\n",
    "                    label_l, label_m)] = learner.predict(X=X_val).flatten()\n",
    "\n",
    "\n",
    "    return preds_dict_train, preds_dict_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "174px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
